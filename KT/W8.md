### Classification

+ Definition
	- predicting a discrete class or classes, those classes are defined in advance.

+ Binary (weather prediction)
+ Multi-class
	- news, articles

+ What are (Supervised) Classifiers?
	- The goal of learning from examples is not to memorise but rather to generalise, e.g., predict.

### Methods
#### 1. Linear regression
+ captures a relationship between two variables or attributes.

+ Assumption:
	- there is a linear relationship between the two variables.
+ Applicability
	- Regression can be applied when all variables/attributes are real numbers.
+ Fitting the model
	- minimize the predicted value with the original label.
	- Minimize the RSS(Residual Sum of Squares)
+ Prediction
	- finding the right beta(s)

#### 2. K-Nearest Neighbour methods in Classification
+ Given class assignments for existing data points, classify a new point
(black).
	- (a) According to the class membership of the K closest data points.	
	- (b) For k = 1, the induced decision boundary

+ kâˆ’Nearest Neighbour classification strategies
	- **[1-NN]**
		- cloest training instance
	- **[k-NN]**
		- majority of k instances
	- **[weighted k-NN]**
		- weights are based on similarity of the input to each of the k neighbours
	- **[offset-weighted k-NN]**
		- weighted k-NN + factoring in an offset to indicate the prior expectation of a test input being classified as being a member of that class.
		- e.g. probability of each class
+ Implementation
	- 1) Brute-Force neighours search
		- O(DN^2)
	- 2) Tree-based data structures
		- O(DNlong(N))
		- The basic idea is that if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant, without having to explicitly calculate their distance.

+ Strengths
	- simple
	- Can handle arbitrarily many classes(multi-class and multi-label)

+ Weaknesses
	- highly relies on distance and by using different distance funcitons, results may change
	- sometimes calculating average does not make sense
		- eg. In categorical data, what is the AVG between sunny and overcast?
	- Expensive
	- lazy learner
		- every time we have a new instance, we have to calculate all the pair distances
	- prone to bias
		- if the data is so noisy, then the performance of k-NN drops significantly
	- Arbitrary k value
		- \# of K is not clear
			- **use odd number**
			- **EVEN number should not be used**

#### 3. Bayesian Methods
+ https://blog.csdn.net/sinat_36246371/article/details/60140664
+ Definition
	- Learning and classification methods based on **probability theory**
	- Build a **generative model** that approximates how data is produced
	- Categorisation produces a posterior probability distribution over the possible categories given a description of an instance. 

+ Naive Bayes(NB) Classifiers
	- 













