+ https://machinelearningmastery.com supervised-and-unsupervised-machine-learning-algorithms/

## Lecture 12 Clustering
### Clustering
	- unsupervised
	- the class of an example is not known
	- finding groups of items that are similar 


### Basic Contrasts
+ Exclusive vs. overlapping clustering
	- Can an item be in more than one cluster?
+ Deterministic vs. probabilistic clustering (Hard vs. soft clustering)
	- Can an item be partially or weakly in a cluster?
+ Hierarchical vs. partitioning clustering
	- Do the clusters have subset relationships between them? e.g. nested in a tree?
+ Partial vs. complete
	- In some cases, we only want to cluster some of the data
+ Heterogenous vs. homogenous
	- Clusters of widely different sizes, shapes, and densities
+ Incremental vs. batch clustering
	- Is the whole set of items clustered in one go?

### properties
+ scalability
+ deal with diff types of attributes
+ noise and outliers
+ insentitive to order of input records

### Types of Evaluation
+ Unsupervised
	- Measures the goodness of a clustering structure without respect to external information.
+ Supervised
	- Measures the extent to which the clustering structure discovered by a clustering algorithm matches some external structure.
+ Relative
	- Compares different clusterings or clusters

+ Most common measure is **Sum of Squared Error (SSE)** or **Scatter** 

### Methods
+ Similarity/proximity/closeness
	- Euclidean, Manhattan
	- Discrete values
		- Hamming distance
	- Docs
		- Cosine similarity
		- Jaccard measure
	- Others
		- Correlation
		- Graph-based measures

#### K-means Clustering 38:58
+ select k points to act as seed cluster centroids
+ repeat
	- Assign each instance to the cluster with the nearest centroid
	- Recompute the centroid of each cluster
+ until the centroids don't change
+ Exclusive, deterministic, partitioning, batch clustering method

+ **PROS and CONS**

#### Hierarchical Clustering
+ Bottom-up (= agglomerative) clustering
+ Top-down (= divisive) clustering




## Lecture 13: Evaluation

### Evaluation
+ Tensions in Classification
	- Generalisation
		- how well the classifier gernealize from the specifics of the training examples to predict the target function?
	- Overfitting
		- is it overfitting?
	- Consistency
		- can it flawlessly predict the class of all training instances?

+ Generalization problem in Classification
	- **Under-fitting**: 
		- model not expressive enough to capture patterns in the data.
	- **Over-fitting**: 
		- model too complicated; capture noise in the data.
	- **Appropriate-fitting**:
		- model captures essential patterns in the data.

+ Evaluation Process
	- 80% for training
	- 20% for validating(testing)

+ **Learning Curves**
	- represent the performance of a fixed learning strategy over different sizes of training data, relative to a fixed evaluation metric.

	- **Inductive Learning Hypothesis**
		- for training data with high scale, if the strategy fits training data well, it will have high probability that the strategy also fit the testing data well.

+ 4 scenarios that a classifier may classify:

<img src="4 scenarios of prediction">

#### Measures
+ Classification accuracy
	- the proportion of instances for which we have correctly predicted the label, which corresponds to:
	- **ACC** = (TP + TN) / (TP + FP + FN + TN)
	- **ER** = 1 - ACC ; ER means error rate
+ ERR: error rate reduction, comparing the error rate ER for a given method with that for a benchmark/baseline method ER0:
	- **ERR** = (ER(0) - ER) / ER(0)

+ Precision and Recall over two categories(T & F)
	- Motivation: wonder the  performance relative to a single class of interest
	- **Precision**: positive predictive value
		- Proportion of positive predictions that are correct
		- TP / (TP + FP)
	- **Recall** = sensitivity
		- Accuracy with respect to positive cases (true positive rate)
		- TP / (TP + FN)

	- **Specificity**
		- the accuracy with respect to negative cases
		- TN / (TN + FP)

+ Precision and Recall over Multiple Categories
	- Micro: all the samples over all classes
	- Macro: calcualte for each class first and then average all classes
	- <img src="Micro and Macro averaging">

	- Example:
		- we have 3 classes and 100 samples
		- class 1 has 50 samples, class 2 has 45 samples and c3 only has 5 samples
		- if use micro averaging and the strategy can accruately predict class 1 and 2, the precision is 95%!!!
		- micro => assign same weight on each class

##### F-Score
+ Making individual decisions for each data point rather than generating a monolithic ranking
+ F-score gives us an overall picture of system performance:
	- <img src="F-score" width="600" height="444">
+ https://www.zhihu.com/question/30643044



##### ROC and AUC
+ ROC - Receiver Operating Characteristic
	- TPR vs. FPR
	- (recall/sensitivity) vs. (1- specificity)
	- (0,1) upper left corner
+ AUC - Area Under the Curve
	- better if closer to 100%


#### Model Validation
##### Holdout 
+ The (training) bias of a classifier is the **average distance between
the expected value and the estimated value**
+ The (test) variance of a classifier is the standard deviation between
the estimated value and the average estimated value
+ The noise in a dataset is the inherent variability of the training data

+ Advantages:
	- simple to work with
	- high reproducibility
+ Disadvantages:
	- trade-off between more training and more test data (variance vs. bias)
	- representativeness of the training and test data

##### Random Subsampling
+ Perform holdout over multiple iterations, randomly selecting the
training and test data (maintaining a fixed size for each dataset) on
each iteration

##### Leave-One-Out
https://www.zhihu.com/question/23561944

##### M-fold Cross-Validation

+ Good points:
	- We need to train the system only M times unlike Leave-One-Out which requires training N times.
	- We can measure the stability of the system across different
training/test combinations.
+ Bad points:
	- There can be a bias in evaluating the system due to sampling, how data is distributed among the M partitions.
	- The results will not be unique unless we always partition the data identically. One solution is repeat the M Fold Cross Validation by randomly shuffling the data M/2 times.
	- The results will give slightly lower accuracy values as only Mâˆ’1 M of the data is used for training.
	- For small data sets it is not always possible to partition the data properly such that each partition represents the data IID (Identically Independently Distributed).






















