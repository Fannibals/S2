+ https://machinelearningmastery.com supervised-and-unsupervised-machine-learning-algorithms/

## Lecture 12 Clustering
### Clustering
	- unsupervised
	- the class of an example is not known
	- finding groups of items that are similar 


### Basic Contrasts
+ Exclusive vs. overlapping clustering
	- Can an item be in more than one cluster?
+ Deterministic vs. probabilistic clustering (Hard vs. soft clustering)
	- Can an item be partially or weakly in a cluster?
+ Hierarchical vs. partitioning clustering
	- Do the clusters have subset relationships between them? e.g. nested in a tree?
+ Partial vs. complete
	- In some cases, we only want to cluster some of the data
+ Heterogenous vs. homogenous
	- Clusters of widely different sizes, shapes, and densities
+ Incremental vs. batch clustering
	- Is the whole set of items clustered in one go?

### properties
+ scalability
+ deal with diff types of attributes
+ noise and outliers
+ insentitive to order of input records

### Types of Evaluation
+ Unsupervised
	- Measures the goodness of a clustering structure without respect to external information.
+ Supervised
	- Measures the extent to which the clustering structure discovered by a clustering algorithm matches some external structure.
+ Relative
	- Compares different clusterings or clusters

+ Most common measure is **Sum of Squared Error (SSE)** or **Scatter** 

### Methods
+ Similarity/proximity/closeness
	- Euclidean, Manhattan
	- Discrete values
		- Hamming distance
	- Docs
		- Cosine similarity
		- Jaccard measure
	- Others
		- Correlation
		- Graph-based measures

#### K-means Clustering 38:58
+ select k points to act as seed cluster centroids
+ repeat
	- Assign each instance to the cluster with the nearest centroid
	- Recompute the centroid of each cluster
+ until the centroids don't change
+ Exclusive, deterministic, partitioning, batch clustering method

+ **PROS and CONS**

#### Hierarchical Clustering
+ Bottom-up (= agglomerative) clustering
+ Top-down (= divisive) clustering



## Lecture 13: Evaluation

### Evaluation
+ Tensions in Classification
	- Generalisation
		- how well the classifier gernealize from the specifics of the training examples to predict the target function?
	- Overfitting
		- is it overfitting?
	- Consistency
		- can it flawlessly predict the class of all training instances?

+ Generalization problem in Classification
	- **Under-fitting**: 
		- model not expressive enough to capture patterns in the data.
	- **Over-fitting**: 
		- model too complicated; capture noise in the data.
	- **Appropriate-fitting**:
		- model captures essential patterns in the data.

+ Evaluation Process
	- 80% for training
	- 20% for validating(testing)

+ **Learning Curves**
	- represent the performance of a fixed learning strategy over different sizes of training data, relative to a fixed evaluation metric.

	- **Inductive Learning Hypothesis**
		- for training data with high scale, if the strategy fits training data well, it will have high probability that the strategy also fit the testing data well.

+ 4 scenarios that a classifier may classify:

<img src="4 scenarios of prediction">

#### Measurement
+ Classification accuracy
	- the proportion of instances for which we have correctly predicted the label, which corresponds to:
	- **ACC** = (TP + TN) / (TP + FP + FN + TN)
	- **ER** = 1 - ACC ; ER means error rate
+ ERR: error rate reduction, comparing the error rate ER for a given method with that for a benchmark/baseline method ER0:
	- **ERR** = (ER(0) - ER) / ER(0)

+ Precision and Recall over two categories(T & F)
	- Motivation: wonder the  performance relative to a single class of interest
	- **Precision**: positive predictive value
		- Proportion of positive predictions that are correct
		- TP / (TP + FP)
	- **Recall** = sensitivity
		- Accuracy with respect to positive cases (true positive rate)
		- TP / (TP + FN)

	- **Specificity**
		- the accuracy with respect to negative cases
		- TN / (TN + FP)

+ Precision and Recall over Multiple Categories
	- Micro: all the samples over all classes
	- Macro: calcualte for each class first and then average all classes
	- <img src="Micro and Macro averaging">

	- Example:
		- 



















