## Information Retrieval

### 1. Definition
+ “the subfield of computer science that deals with storage and retrieval of **documents**

+ Data retrieval vs. Information retrieval
	- Data retrieval
		- the information is unambiguous
		- usually typical information
		- searched through query

	- Information retrieval
		- docs are rich and ambiguous

	- IR emphasis on the **user**
	- The **meaning** or **content** of a document is of more interest than the specific words used to express the meaning.
+ Data retrieval
	 -Data retrieval means obtaining data from a database management system such as ODBMS. In this case, it is considered that data is represented in a structured way, and there is **no ambiguity in data**

+ Information retrieval 
	- IR system is concerned with the document as originally created, not with a formal representation of the document
	- Documents are rich and ambiguous
	- Users may not agree on the value of a particular document, even in relation to the same query
	
### 2. Document Collections
+ Documents aren’t always text.  
	- They can be defined as **messages**: an object that conveys information from one person to another.
	- docs in IR can be e text, images, music, speech, handwriting, video, and genomes.
+ Inforamtion needs
	- The different kinds of IR system are linked by the concept of information need.
### 
+ User has an **information need**
+ User formulates a **query**
+ IR engine **retrieves** a set of documents


### Answer
+ An answer to a query can be defined as a document that matches the query according to formal criteria: if it contains all the query words, it can be described as a match. 
+ Relevant is important. 
	- Relevance can be defined as: a doc is relevant if it contains knowledge that helps user to resolve the information need.

+ some improvements on relevance 
	- a snippet
	- prune duplicates
	- a single source might only contribute a single answer

### Approaches to retrieval
+ a human would see the query as representative of a topic, and evaluate docs accordingly.
+ we have to develop methods that use other forms of evidence to make a **guess as to whether a doc is relevant**.

## Document Matching 

### Boolean querying
+ repeatable, auditable and controllable
+ allows expression of complex concepts
+ conpensation between the length of query and the number of docs searched 
+ not satisfaction due to
	- no ranking
	- no control over result set size
	- difficult to incorporate useful heuristics
+ Example
	- **diabetes & risk & factor & NOT juvenile**
	- docs match if they contain these terms and not contain the NOT terms
	- NO ordering , there is only YES/NO
 
+ 
| | doc1| doc2| doc3|
|--|--|--|--|
| juvenile| 1| 0|0|
| diabetes|1 |1 |0|
|risk|0|1|1|
|factor|0|1|1|

+ bit representation and AND
		- diabetes = 110; risk = 011
		- 110 AND 011 ==> 010
		- doc 2 is the only match

	+ disjunction ==> OR 
	+ negation ==> ^
 
+ Rankings
	- based on simailarity
	-The more similar or likely a document is, relative to the other documents in the collection, the higher its rank.

+ Evidence of Similarity
	- Choose docs with words in common with the query
		- the most significant word
	- where terms appear? body or title(may better)
	- time created
	- more reliable, authoritative
	- sequence : abc / bac for searching abc

+ Heuristics in Similarity measurement
	- **TF-IDF model**
	- less weight is given to a **term** that appears in many docs.
		- **Inverse document frequency --IDF**
	- More weight is given to a document where a query term appears many times.
		- **Term frequency -- TF**
	- Less weight is given to a **document** that has many terms.

### Principles and Models
+ The vector-space model
	- A vector locates a doc as a point in n-space
	- docs with a similar distribution of terms have similar angles in the space.
	- Typical problems
		- not clear to find weighting function w
		- vector space -- orthogonal formulation
+ P36
+ A typical strategy is to find the cosine of the angle between two vectors
	- one defined by the document and one defined by the query
	- P38 traditional 
	- P39 log method (decrease the large-number effect)
	- P40	sum rather than multiplication 

+ calculation

### Evaluation Metrics
+ Evaluatio in information Retrieval
	- have one or more queries
	- system return many docs
	- exam the relevance 
		- precision 
		- recall
+ **compare with Approx. Search"
	- Typically many more results in IR than Approx. Search
	- IR has multiple "correct"(relevant) results; Approx. Search only one
	- IR results are ranked, Approx. Search typically not**

+ Precision **
	- \# of returned relevant results / # of returned results
+ Recall
	- \# of returned relevant results / totoal # of relevant results

+ Precision at k
	- \# of returned relevant results in top k / k
+ Average Precision
	- (1/R)*(sum of P@K)*









