## Support Vector Machines(SVMs)
+ One of the most famous classification methods
### Intuition
+ Assuming the data is linearly separable
+ Aim: Finding a linear hyperplane that will seperate the data

### Large Margin Classifiers
+ Should Find hyperplane maximises the margin
+ Margin: sum of the shorest distances from the planes to the positive/negative samples

+ WHY?
  - Small margin seperating planes:
    - more fragile to noise
    - may over-fit the data
  - Large one:
    - more robust to noise
    - large margin planes generalises better to unseen data(Statistical learning theory)
    
### Linear Classifiers Formulation
+ {xi,yi}
  - xi is the training instance in D-dimentional space 
  - D features == D-dimentional
  - yi = {-1,1} two classes
  - i = 1...L (L samples)

#### Formula
+ x\* w + b = 0 
  - represent the hyperplane
+ **w** is normal to the hyperplane
  - w 垂直于超平面
+ b/|w| 
  - perpendicular distance from the hyperplane to the origin

+ classification rule -- training 
  - assign classes -1 or 1 to each training instances
  - left part as -1
  - right part as +1

#### Training objective
+ **Find w and b such that 
  - xi\*w + b **>=** 0 for yi = +1
  - xi\*w + b **<** 0 for yi = -1
  - for all i = 1...L

#### Then how?
+ Objective: **Need to find the maximum margin**
+ First of all, find the maximum margin through **Support Vectors**
+ assign two support vectors as +1 and -1
  - recall our training objective:
+ Hence, max 2/|w|
  - Subject to(st.):
  - xi\*w + b **>=** +1 for yi = +1
  - xi\*w + b **<=** -1 for yi = -1
  - **combine these two to:**
    - yi(xi\*w+b)-1>=0 for all i (1.1)

+ Equivalent formulations   
  - max(2/|w|) s.t. 1.1
  - min|w|  s.t. 1.1
  - min\*(1/2)\*|w|^2 s.t. 1.1 - (1.2)

+ (1.2) becomes the convex quadratic optimization problem
  - Solving the optimization Problem
    - Convex object: any local minimum is also the global minimum
    - Non Convex

+ Lagrange multipliers
  
<img src="duality" width="400" height="650">>


+ **Solution: Support Vectors**
  - f(x) = ∑ai\*yi\*xi\*^T\*x + b 

+ **What you SHOULD KNOW!!!**
  - **discriminate primal(more difficult to solve) and dual formulation**
  - what are the main parameters in each of them
  - what is their RULE

+ Feasibility
  - For linearly separable data: 
    - a max-margin solution is guaranteed to exist
  - For non- linearly separable data: 
    - a solution does not exist

### Soft Margin Classification
+ The (hard) linear SVM problem is **infeasible** for:
  - if the training set is mostly, but not exactly, linearly separable
  - soft one
+ **Slack variables ξi** can be added to 
  - allow misclassification of difficult or noisy examples, 
  - **resulting margin called soft**.

+ Slack variables ξi is the *degree of error* for each sample
+ Σξi is an upper bound on the number of training errors
  - for instance
    - 0 for samples at the right side
    - larger than 0 for samples that are noisy or on the wrong side


+ Φ(w) =wTw + CΣξi
  - and for all (xi ,yi), i=1..L : yi (wTxi + b) ≥ 1 – ξi, , ξi ≥ 0

+ **Parameter C** can be viewed as a way to control overfitting: 
+ larger C corresponding to assigning a higher penalty to errors.
  - it “trades off” the relative importance of maximizing the margin and fitting the training data.
  - if C is too large ==> overfitting

### Model complexity
+ depends on # support vectors
  - higher, more complex

### Linear to Non-linear
+ limitation of linear SVM
+ By mapping the original data into a high dimensional space where the data is
*hopefully* linearly separable
+ General idea:
  -  the original feature space can be mapped to some higherdimensional feature space where the training set is separable



### Non-linear SVMs Overview
+ Φ: x → φ(x)
  - explicit transform function
+ The inner The inner products
  - Can be regarded as a measure of similarity between data points (think cosine
  similarity)

### Calculating inner product by using Kernel Functions
+ A kernel function is a function that is equivalent to an inner product in some feature space.
+ Thus, a kernel function implicitly maps data to a high-dimensional space (without the need to compute each φ(x) explicitly).
  - Save computational cost
  - The target space can have very high dimensionality

+ Mercer's theorem 
  - Every positive semi-definite symmetric function is a kernel


+ Examples
  - Linear:
    - K(xi,xj)= xiTxj
    - Mapping Φ: x → φ(x), where φ(x) is x itself
  - Polynomial of power p:
    - K(xi,xj)= (1+ xiTxj)^p
    - Mapping Φ: x → φ(x), where φ(x) has (d+p .. p) dimensions   
      - d - true dimentions
      - p - power of function

### Gaussian (Radial-Basis Function (RBF))


### In Practice
+ How to find the suitable kernel function and its parameters?
  - Method: Using M-fold cross-validation error rate

### Multi-class Extension
+ SVM is inherently a binary classifier
+ Extension to multiclass:
  - One-versus-all: build M classifiers for M classes. Choose class with largest margin for test data
  - One-versus-one: one classifier per pair of classes (M(M-1)/2 classifiers in total), choose class selected by most classifiers


#### references and websites
+ https://www.zhihu.com/question/21094489

+ Primal problem: solve for w and b
+ Equivalent dual problem formulation:
  - solve for a1,a2...al

+ Text P417
+ http://www.sohu.com/a/206572358_160850

+ https://blog.csdn.net/fengshuiyue/article/details/43482533
+ https://www.zhihu.com/question/21094489





