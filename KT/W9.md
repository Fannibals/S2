## Support Vector Machines(SVMs)
+ One of the most famous classification methods
### Intuition
+ Assuming the data is linearly separable
+ Aim: Finding a linear hyperplane that will seperate the data

### Large Margin Classifiers
+ Should Find hyperplane maximises the margin
+ Margin: sum of the shorest distances from the planes to the positive/negative samples

+ WHY?
  - Small margin seperating planes:
    - more fragile to noise
    - may over-fit the data
  - Large one:
    - more robust to noise
    - large margin planes generalises better to unseen data(Statistical learning theory)
    
### Linear Classifiers Formulation
+ {xi,yi}
  - xi is the training instance in D-dimentional space 
  - D features == D-dimentional
  - yi = {-1,1} two classes
  - i = 1...L (L samples)
+ **w** is normal to the hyperplane
  - w 垂直于超平面
+ b/|w| => perpendicular distance from the hyperplane to the origin

+ classification rule -- training 
  - assign classes -1 or 1 to each training instances
+ Need to find the maximum margin

+ https://www.zhihu.com/question/21094489


+ Linear SVM equivalent Formulations
  - max(2/|w|) s.t. yi(xi*w+b)-1>=0 -vi
  - min|w|  s.t. same as above
  - min*(1/2)*|w|^2 s.t. same as above
  
+ Solving the optimization Problem
  - Convex object: any local minimum is also the global minimum
  - Non Convex

+ Primal problem: solve for w and b
+ Equivalent dual problem formulation:
  - solve for a1,a2...al

+ Text P417
+ http://www.sohu.com/a/206572358_160850

+ https://blog.csdn.net/fengshuiyue/article/details/43482533
+ https://www.zhihu.com/question/21094489



